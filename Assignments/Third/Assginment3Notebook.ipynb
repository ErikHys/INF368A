{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Assignments.Third.Model import MyFFLM, cross_entropy\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "with open(\"lotrFotr.txt\",\"r\",encoding='utf-8') as file:\n",
    "    raw_txt = file.read()\n",
    "\n",
    "stripped_txt_lotr = raw_txt.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ')\\\n",
    "    .replace(';', '').replace(':', '').replace('  ', ' ').replace('.', '').replace(',', '').replace('\"', '').lower()\n",
    "stripped_txt_lotr = re.sub(r'[^\\w\\s]', '', stripped_txt_lotr).split()\n",
    "\n",
    "# with open(\"mobydick.txt\",\"r\",encoding='utf-8') as file:\n",
    "#     raw_txt = file.read()\n",
    "#\n",
    "# stripped_txt_mb = raw_txt.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ')\\\n",
    "#     .replace(';', '').replace(':', '').replace('  ', ' ').replace('.', '').replace(',', '').lower().split()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9794\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(stripped_txt_lotr))\n",
    "vocab.sort()\n",
    "vocab_dict = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    identity_vector = np.zeros(len(vocab))\n",
    "    identity_vector[i] = 1\n",
    "    vocab_dict[word] = identity_vector\n",
    "print(len(vocab))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Need to split up bc of ram issues, batches is the answer I think\n",
    "memory_depth = 3\n",
    "dataX = np.zeros((len(stripped_txt_lotr[:1000]) - memory_depth, memory_depth, len(vocab_dict)))\n",
    "dataY = np.zeros(len(stripped_txt_lotr[:1000]) - memory_depth, dtype='int32')\n",
    "for i in range(len(stripped_txt_lotr[:1000]) - memory_depth):\n",
    "    x = np.zeros((memory_depth, len(vocab_dict)))\n",
    "    for j in range(memory_depth):\n",
    "        x[j] = vocab_dict[stripped_txt_lotr[:1000][i+j]]\n",
    "    dataX[i] = x\n",
    "    dataY[i] = np.argmax(vocab_dict[stripped_txt_lotr[:1000][i+memory_depth]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "rings\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "for\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "the\n",
      "8463\n"
     ]
    }
   ],
   "source": [
    "reversed_dict = {}\n",
    "for key in vocab_dict:\n",
    "    reversed_dict[np.argmax(vocab_dict[key])] = key\n",
    "for x in dataX[0]:\n",
    "    print(reversed_dict[np.argmax(x)])\n",
    "    print(x)\n",
    "print(reversed_dict[dataY[0]])\n",
    "print(dataY[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch progression 0/3:   3%|â–Ž         | 1575/59997 [00:51<32:22, 30.08it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-6-39a6be67a8be>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     20\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mk\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdataX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdataY\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m             \u001B[0my_pred\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 22\u001B[1;33m             \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackprop\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     23\u001B[0m             \u001B[0mbatch_progress\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m         \u001B[0mbatch_progress\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\INF368A\\Assignments\\Third\\Model.py\u001B[0m in \u001B[0;36mbackprop\u001B[1;34m(self, y_true, y_pred, dloss)\u001B[0m\n\u001B[0;32m     93\u001B[0m         \u001B[0mdl_da0\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0md\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     94\u001B[0m         \u001B[1;31m# print(dl_da0, dl_da0.shape, \"unused embedding bias grad\")\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 95\u001B[1;33m         \u001B[0mdl_dz0\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmatmul\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdl_da0\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'-1'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmemory_depth\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     96\u001B[0m         \u001B[1;31m# print(dl_dz0, dl_dz0.shape, 'embedding grad')\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     97\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "v_len = len(vocab)\n",
    "data_set_length = len(stripped_txt_lotr) - memory_depth\n",
    "batch_size = 60000\n",
    "model = MyFFLM(v_len, 64, learning_rate=0.001, memory_depth=memory_depth)\n",
    "# loss = 0\n",
    "for epoch in range(10):\n",
    "    for batch in range(data_set_length // batch_size):\n",
    "        dataX = np.zeros((batch_size - memory_depth, memory_depth, v_len), dtype='float16')\n",
    "        dataY = np.zeros((batch_size - memory_depth, v_len), dtype='int16')\n",
    "        for i in range(batch_size - memory_depth):\n",
    "            x = np.zeros((memory_depth, len(vocab_dict)))\n",
    "            for j in range(memory_depth):\n",
    "                x[j] = vocab_dict[stripped_txt_lotr[(batch*batch_size):(batch*batch_size)+batch_size][i+j]]\n",
    "            dataX[i] = x\n",
    "            dataY[i] = vocab_dict[stripped_txt_lotr[(batch*batch_size):(batch*batch_size)+batch_size][i+memory_depth]]\n",
    "        # if epoch == 0:\n",
    "        #     for words, target in zip(dataX[995:], dataY[995:]):\n",
    "        #         print([reversed_dict[np.argmax(word)] for word in words], reversed_dict[np.argmax(target)])\n",
    "        batch_progress = tqdm(total=dataX.shape[0], desc=\"batch progression {}/{}\".format(batch, data_set_length // batch_size))\n",
    "        for k, (x, y) in enumerate(zip(dataX, dataY)):\n",
    "            y_pred = model.forward(x)\n",
    "            model.backprop(y, y_pred)\n",
    "            batch_progress.update()\n",
    "        batch_progress.close()\n",
    "            # if k % (batch_size - memory_depth - 1) == 1 and  k != 1:\n",
    "            #     print(\"epoch:\", epoch, \"batch:\", f\"{batch}/{data_set_length//batch_size}\")\n",
    "            #     l = cross_entropy(y, y_pred)\n",
    "            #     print(\"loss:\", l, l[0, np.argmax(l)])\n",
    "            #     print(\"Predicted vs actual:\", reversed_dict[np.argmax(y_pred)], reversed_dict[np.argmax(y)])\n",
    "            #     print(\"Predicted value:\", y_pred[0, np.argmax(y_pred)])\n",
    "        print(batch)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}