{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Assignment 1 Erik Hystad\n",
    "Contents:\n",
    "    * Code: CorrectionTool.py, MyBigramFilterFinder.py  -> independent programs that can be used elsewhere\n",
    "    * Code/report: Assignment_1_Erik_Hystad.ipynb   -> report/implementation\n",
    "    * Data: 1_1_1.txt, 1_1_2.txt    -> Collocations from the first task\n",
    "\n",
    "\n",
    "Requirements:\n",
    "\n",
    "Nltk\n",
    "    * Corpus.Brown\n",
    "    * Corpus.Wordnet\n",
    "    * Collocations\n",
    "Jupyter Notebook, not necessary if you are using .py files.\n",
    "\n",
    "Run all cells to get results, or if you have text files, skip to last cell for correction tool."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Python files\n",
    "Alternatively there is 2 python files with classes you can run to use in other programs.\n",
    "\n",
    "\n",
    "##### Task 1.1/MyBigramFilterFinder.py\n",
    "\n",
    "MyBigramFilterFinder -> bigram_filter = MyBigramFilterFinder() <br>\n",
    "bigram_filter.get_hypothesis_tested_bigrams -> returns a list of hypothesis tested bigrams<br>\n",
    "bigram_filter.get_freq_and_noun_adj_filtered -> returns a list of bigrams\n",
    "\n",
    "\n",
    "##### Task 1.2/CorrectionTool.py\n",
    "\n",
    "CorrectionTool.py -> correction_tool = CorrectionTool(file_path_of_collocation_library)<br>\n",
    "correction_tool.correct(first_word, second_word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import math\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.collocations import *\n",
    "from nltk.corpus import wordnet\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 1.1\n",
    "I need to find, filter and test all collocations.\n",
    "First I will load the corpus and find all collocations with the BigramCollocationFinder."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "corpus = brown.words()\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(corpus)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Task 1.1.1\n",
    "Then I will apply a frequency filter to the finder, which I here set to 6, to remove any collocations that\n",
    "appear less than 6 times. I chose 6 to reduce somewhat the time the hypothesis testing in 1.1.2 would take to run.\n",
    "Then I only keep collocations that are made up of nouns and adjectives.\n",
    "\n",
    "After that I write these to a file, '1.1.1.txt'."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "finder.apply_freq_filter(6)\n",
    "frequency_collocations = finder.nbest(bigram_measures.pmi, 10000)\n",
    "\n",
    "\n",
    "tagged = [nltk.pos_tag(bigram) for bigram in frequency_collocations]\n",
    "including = ['NN', 'JJ']\n",
    "\n",
    "\n",
    "# A function to see if a word is nouns or adjectives, will include NNS etc...\n",
    "def check(bigram):\n",
    "    for _, cl in bigram:\n",
    "        result = False\n",
    "        for cls in including:\n",
    "            if cls in cl:\n",
    "                result = True\n",
    "        if not result:\n",
    "            return result\n",
    "    return True\n",
    "\n",
    "\n",
    "noun_and_adjectives_collocations = [bigram for bigram in tagged if check(bigram)]\n",
    "\n",
    "path_1 = \"1_1_1.txt\"\n",
    "with open(path_1, 'w') as file:\n",
    "    for bigram in noun_and_adjectives_collocations:\n",
    "        file.write(bigram[0][0] + ' ' + bigram[1][0] + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Task 1.1.2\n",
    "\n",
    "Here I did hypothesis testing on the remaining bigrams, the ones already filtered by nouns, adjectives and more than\n",
    "6 occurrences.\n",
    "\n",
    "For each bigram: <br>\n",
    "&emsp; if t > confidence:  Where t = (sample mean - mean of the dist) / squareroot(p * (1 - p) / corpus length):<br>\n",
    "&emsp;&emsp; Write bigram to file\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Hypothesis test:   0%|          | 0/960 [00:00<?, ?it/s]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 Open\n",
      "0.0 Find\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Hypothesis test:   0%|          | 1/960 [00:02<38:48,  2.43s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4277853965759277 Mean\n",
      "0.0 T\n",
      "0.0 Open\n",
      "0.0 Find\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Hypothesis test:   0%|          | 2/960 [00:04<38:31,  2.41s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.402179479598999 Mean\n",
      "0.0 T\n",
      "0.0 Open\n",
      "0.0 Find\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Hypothesis test:   0%|          | 3/960 [00:07<38:05,  2.39s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3562331199645996 Mean\n",
      "0.0 T\n",
      "0.0 Open\n",
      "0.0 Find\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Hypothesis test:   0%|          | 4/960 [00:09<37:52,  2.38s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3601434230804443 Mean\n",
      "0.0 T\n",
      "0.0 Open\n",
      "0.0 Find\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-5-7ca059236fb8>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     25\u001B[0m     \u001B[0mpbar\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtotal\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnoun_and_adjectives_collocations\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdesc\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'Hypothesis test'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mbigram\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mnoun_and_adjectives_collocations\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 27\u001B[1;33m         \u001B[1;32mif\u001B[0m \u001B[0mhypothesis_test\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbigram\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbigram\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     28\u001B[0m             \u001B[0mfile\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbigram\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m' '\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mbigram\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m'\\n'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m         \u001B[0mpbar\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-5-7ca059236fb8>\u001B[0m in \u001B[0;36mhypothesis_test\u001B[1;34m(collocation, confidence)\u001B[0m\n\u001B[0;32m     12\u001B[0m     \u001B[0mt3\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mt3\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mt2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"Find\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 14\u001B[1;33m     \u001B[0mmean_of_the_dist\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfirst\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m/\u001B[0m \u001B[0mn\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m*\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msecond\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m/\u001B[0m \u001B[0mn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     15\u001B[0m     \u001B[0mt4\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mt4\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mt3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"Mean\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\erik_\\pycharmprojects\\deeplearning\\venv\\lib\\site-packages\\nltk\\collections.py\u001B[0m in \u001B[0;36mcount\u001B[1;34m(self, value)\u001B[0m\n\u001B[0;32m    182\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mcount\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    183\u001B[0m         \u001B[1;34m\"\"\"Return the number of times this list contains ``value``.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 184\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0msum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0melt\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0melt\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    185\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    186\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mindex\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstart\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstop\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\erik_\\pycharmprojects\\deeplearning\\venv\\lib\\site-packages\\nltk\\collections.py\u001B[0m in \u001B[0;36m<genexpr>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    182\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mcount\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    183\u001B[0m         \u001B[1;34m\"\"\"Return the number of times this list contains ``value``.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 184\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0msum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0melt\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0melt\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    185\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    186\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mindex\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstart\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstop\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\erik_\\pycharmprojects\\deeplearning\\venv\\lib\\site-packages\\nltk\\corpus\\reader\\util.py\u001B[0m in \u001B[0;36miterate_from\u001B[1;34m(self, start_tok)\u001B[0m\n\u001B[0;32m    418\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    419\u001B[0m             \u001B[1;31m# Get everything we can from this piece.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 420\u001B[1;33m             \u001B[1;32mfor\u001B[0m \u001B[0mtok\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mpiece\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0miterate_from\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstart_tok\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0moffset\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    421\u001B[0m                 \u001B[1;32myield\u001B[0m \u001B[0mtok\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    422\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\erik_\\pycharmprojects\\deeplearning\\venv\\lib\\site-packages\\nltk\\corpus\\reader\\util.py\u001B[0m in \u001B[0;36miterate_from\u001B[1;34m(self, start_tok)\u001B[0m\n\u001B[0;32m    302\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_current_toknum\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtoknum\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    303\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_current_blocknum\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mblock_index\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 304\u001B[1;33m             \u001B[0mtokens\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_block\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stream\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    305\u001B[0m             assert isinstance(tokens, (tuple, list, AbstractLazySequence)), (\n\u001B[0;32m    306\u001B[0m                 \u001B[1;34m\"block reader %s() should return list or tuple.\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\erik_\\pycharmprojects\\deeplearning\\venv\\lib\\site-packages\\nltk\\corpus\\reader\\tagged.py\u001B[0m in \u001B[0;36mread_block\u001B[1;34m(self, stream)\u001B[0m\n\u001B[0;32m    330\u001B[0m         \u001B[1;34m\"\"\"Reads one paragraph at a time.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    331\u001B[0m         \u001B[0mblock\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 332\u001B[1;33m         \u001B[1;32mfor\u001B[0m \u001B[0mpara_str\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_para_block_reader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstream\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    333\u001B[0m             \u001B[0mpara\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    334\u001B[0m             \u001B[1;32mfor\u001B[0m \u001B[0msent_str\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sent_tokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpara_str\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\erik_\\pycharmprojects\\deeplearning\\venv\\lib\\site-packages\\nltk\\corpus\\reader\\util.py\u001B[0m in \u001B[0;36mread_blankline_block\u001B[1;34m(stream)\u001B[0m\n\u001B[0;32m    600\u001B[0m     \u001B[0ms\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    601\u001B[0m     \u001B[1;32mwhile\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 602\u001B[1;33m         \u001B[0mline\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstream\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreadline\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    603\u001B[0m         \u001B[1;31m# End of file:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    604\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mline\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\erik_\\pycharmprojects\\deeplearning\\venv\\lib\\site-packages\\nltk\\data.py\u001B[0m in \u001B[0;36mreadline\u001B[1;34m(self, size)\u001B[0m\n\u001B[0;32m   1096\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1097\u001B[0m         \u001B[1;32mwhile\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1098\u001B[1;33m             \u001B[0mstartpos\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstream\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtell\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbytebuffer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1099\u001B[0m             \u001B[0mnew_chars\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_read\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mreadsize\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1100\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "n = len(corpus)\n",
    "\n",
    "\n",
    "def hypothesis_test(collocation, confidence=2.576):\n",
    "    first, second = collocation\n",
    "    sample_mean = (finder.ngram_fd[collocation] / n)\n",
    "    mean_of_the_dist = ((corpus.count(first) / n) * (corpus.count(second) / n))\n",
    "    t = (sample_mean - mean_of_the_dist) / (math.sqrt((sample_mean * (1 - sample_mean))/n))\n",
    "    return t > confidence\n",
    "\n",
    "\n",
    "path_2 = \"1_1_2.txt\"\n",
    "with open(path_2, 'w') as file:\n",
    "    pbar = tqdm(total=len(noun_and_adjectives_collocations), desc='Hypothesis test')\n",
    "    for bigram in noun_and_adjectives_collocations[:10]:\n",
    "        if hypothesis_test((bigram[0][0], bigram[1][0])):\n",
    "            file.write(bigram[0][0] + ' ' + bigram[1][0] + '\\n')\n",
    "        pbar.update()\n",
    "    pbar.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.2\n",
    "Here I will need to be able to take an input, which should be an incorrect collocation, and return\n",
    "a corrected version of the collocation. Run the next cell to try it with an input, or call the correction_tool\n",
    "function with a bigram to try it out.\n",
    "\n",
    "The process of correction tool works as follows:<br>\n",
    "&emsp;    for all synonyms of the first word:<br>\n",
    "&emsp;&emsp;        if synonym exists in the collocation library as a first word:<br>\n",
    "&emsp;&emsp;&emsp;            second_words <- find all collocations that has the first word in the first spot, store second word<br>\n",
    "&emsp;&emsp;&emsp;            for all words in second_words:<br>\n",
    "&emsp;&emsp;&emsp;&emsp;                for all synonyms of the second word(input):  # Not the words we found above, but the second word of the input bigram<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;                    if second_synonym equals second word(input) -> you have found a correction.<br>\n",
    "\n",
    "There are some syntactic sugar with wordnet library, synset.lemmas().name()...\n",
    "\n",
    "Runtime is not optimal here, where\n",
    "    * n: is number of synonyms of the first word\n",
    "    * m: is number of collocations\n",
    "    * p: is number of collocations that start with the first word\n",
    "    * q: is number of synonym each word from second_words (all words in p)\n",
    "runtime: n * (m + p * q)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(path_2, 'r') as file:\n",
    "    raw = file.read()\n",
    "\n",
    "learned_collocations = [(x.split()[0].lower(), x.split()[1].lower()) for x in raw.split('\\n') if x != '']\n",
    "# This is used to check if the first word of the bigram is in the library in O(1) time\n",
    "learned_collocations_firsts = set([x[0] for x in learned_collocations])\n",
    "\n",
    "def correction_tool(first, second):\n",
    "    for synset in wordnet.synsets(first):\n",
    "        for synonym in synset.lemmas():\n",
    "            f_synonym = synonym.name()\n",
    "            if f_synonym in learned_collocations_firsts:\n",
    "                second_learned = [x[1] for x in learned_collocations if x[0] == f_synonym]\n",
    "                for s in second_learned:\n",
    "                    for second_synset in wordnet.synsets(second):\n",
    "                        for second_synonym in second_synset.lemmas():\n",
    "                            s_synonym = second_synonym.name()\n",
    "                            if s == s_synonym and first + ' ' + second != f_synonym + ' ' + s:\n",
    "                                print('Changed', first + ' ' + second, 'to', f_synonym + ' ' + s + '!')\n",
    "                                return f_synonym + ' ' + s\n",
    "    print(\"Found no matching collocation.\")\n",
    "    return \"\"\n",
    "\n",
    "first, second = input(\"Write bigram to see if can be corrected to a collocation. e.g. polite war\").split()\n",
    "correction_tool(first, second)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}